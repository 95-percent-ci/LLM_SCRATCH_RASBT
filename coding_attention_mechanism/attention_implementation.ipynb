{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simplified Attention\n",
    "\n",
    "Say we want to compute self-attention output for 2nd token `x^2` in position, which is `journey`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Sample Toy Input\n",
    "\n",
    "Word embedding are made to be just 3D dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Computing Attention Scores\n",
    "\n",
    "Dot Product operation is used mathematically represent how two vectors are aligned\n",
    "> Dot Product of query `x^2` with every other token in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Normalise Attention Scores\n",
    "\n",
    "* Main Goal behind the normalization to obtain attention weights which can sum to 1\n",
    "\n",
    "* It makes it use full for convention and interpretation\n",
    "\n",
    "We use softmax instead of summation function as it offers\n",
    "\n",
    "* Positive Values -> Probability\n",
    "* Favorable Gradient Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.1. Summation Normalisation [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2. Softmax Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())\n",
    "\n",
    "\n",
    "## using pytorch inbuilt softmax function\n",
    "attn_weights_2 = torch.nn.functional.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Updated Context Vector\n",
    "\n",
    "This final step, updates initial context vector of `x^2` using attention weights derived in previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "## for loops which is slower\n",
    "\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "## matrix multiplication which is faster\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Updated Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self Attention [Q,K,V]\n",
    "\n",
    "This form of self attention is used in Transformer architectures. let's start with same Toy Input\n",
    "\n",
    "On a high level, self attention is way to update input token embeddings to account for context in the sequence. For Example, let's consider 3 Individual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Single Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Defining Query, Key and Value Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Computing Q,K,V for Input Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Key & Value for all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Computing Attention Scores\n",
    "\n",
    "Let `D` be dimension of Q, K, V projection matrices and `N` be sequence length, with Embedding being `P` dimensional\n",
    "\n",
    "\n",
    "Query: Query Vector Of Input Token in Sequence: (1, D)\n",
    "Keys: Key Vector Of all tokens in Sequence: (N, D)\n",
    "Values: Value Vector for all tokens in Sequence: (N, D)\n",
    "\n",
    "Attention_score = (1, D) dot (N, D) --> (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_score_2: tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_score_2 = query_2 @ keys.T\n",
    "print(\"attn_score_2:\", attn_score_2)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Scaling Attention Scores\n",
    "\n",
    "Let $d_k$ be dimension of key vector and query vector (both have to be same), so when we take dot product\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention score would now have variance of $\\sigma^4 . d_k$, In order stablise variance so that it is approximately 1 thorughout the model, attention score is divided by $ \\sqrt{d_k} $ \n",
    "\n",
    "> Note: $ \\sigma^4$ is ignored as it around 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights_2: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / (d_k ** 0.5), dim=-1)\n",
    "print(\"attn_weights_2:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Updating Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Using nn.Paramter for Q, K, V Weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        d = keys.shape[1]\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        scaled_attn_scores = attn_scores / (d ** 0.5)\n",
    "        attn_weights = torch.softmax(scaled_attn_scores, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Using nn.Linear Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        d = keys.shape[1]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "W_value = nn.Linear(d_in, d_out, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "querW_query(inputs).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
